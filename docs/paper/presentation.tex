\section{Presentation of the library}
\label{sec:presentation}

We provide in this section an introduction to the \texttt{\detokenize{time_interpret}} library.
Please also refer to the documentation.

\texttt{\detokenize{time_interpret}} is primarily composed of 4 different parts: attribution methods, datasets,
evaluation tools (metrics) and deep learning models.
We present below a short description of the components in each of these parts.


\paragraph{Attribution methods.}

Attribution methods constitute the core of \texttt{\detokenize{time_interpret}}.
In this part of the library, we regrouped many methods which have been recently published.
Similarly to Captum~\citep{kokhlikyan2020captum}, each method can be called like this:

\begin{lstlisting}[language=Python, caption=Attribution loading example, label={lst:attr}]
from tint.attr import TemporalIntegratedGradients

explainer = TemporalIntegratedGradients(
    model
)
attr = explainer.attribute(inputs)
\end{lstlisting}

where $``$model$"$ is a PyTorch model, and $``$inputs$"$ is an inputs' tensor.

We provide in this library several methods:

\begin{itemize}
    \item \textbf{AugmentedOcclusion}.
        This method improves upon the original Occlusion method from captum~\url{https://captum.ai/api/occlusion.html}
        by allowing to sample the baseline from a bootstrapped distribution.
        By selecting a distribution close to the inputs, the resulted occulted data should be close to actual data as a
        result, limiting the amount of out of distribution samples.
        This method was originally proposed by~\citep{tonekaboni2020went}, Section 4.
    \item \textbf{BayesLime, BayesKernelShap}.
        These two methods, originally proposed by~\citep{slack2021reliable}, extend respectively
        LIME~\citep{ribeiro2016should} and KernelSHAP~\citep{lundberg2017unified}, by replacing the underlying
        linear regression model with a bayesian linear regression, allowing the method to model uncertainty in
        explainability, by outputting credible intervals in addition to the features attributions.
    \item \textbf{DiscretetizedIntegratedGradients (DIG)}.
        DIG~\citep{sanyal2021discretized} was designed to interpret predictions made by language models.
        It builds upon the original Integrated Gradients method by generating discretized paths, hopping from one
        word to another, instead of using straight lines.
        This way, it aims to create a path which takes into account the discreteness of the embedding space.
    \item \textbf{DynaMask}.
        This method, introduced by~\citep{crabbe2021explaining}, is an adaptation of a perturbation-based method
        developed in~\citep{fong2017interpretable, fong2019understanding}, to handle time-series data.
        As such, it consists in perturbing a temporal data by replacing some of it with an average in time.
        The mask used to choose which data should be preserved and which should be replaced is learnt in order to either
        preserve the original prediction with a minimum amount of unmasked data, or change the original prediction with
        a small amount of masked data.
        Either way, the learnt mask can then be used to discriminate between important features and others.
    \item \textbf{ExtremalMask}.
        This method consists in a generalisation of DynaMask, which learns not only the mask, but also the associated
        perturbation, instead of replacing perturbed data with a predetermined average in time.
        Learning perturbations allows this method to take into account eventual long term dependencies, such as
        temporal regularities.
    \item \textbf{Fit}.
        Originally proposed by~\citep{tonekaboni2020went}, this method aims to understand which feature is important by
        quantifying the shift in the predictive distribution over time.
        An important feature is then one which contributes significantly to the distributional shift.
    \item \textbf{LofLime, LofKernelShap}.
        Novel method.
        Please see Section~\ref{sec:methods} for more details.
    \item \textbf{NonLinearitiesTunnel}.
        Novel method.
        Please see Section~\ref{sec:methods} for more details.
    \item \textbf{Retain}.
        This method~\citep{choi2016retain} uses two RNNs whose output are then used as keys and queries in a attention
        mechanism, also using the original embeddings of the input as values.
        This attention mechanism can then be used to explain which feature was important to make a specific prediction.
    \item \textbf{SequentialIntegratedGradients (SIG)}.
        SIG is an adaptation of the Integrated Gradients method to sequential data.
        It modifies the baseline by only masking one element of a sequence at a time, and computing the corresponding
        feature attribution.
        By doing so, it allows the baseline to be closer to the original sequence.
    \item \textbf{TemporalOcclusion}.
        Originally proposed by~\citep{tonekaboni2020went}, this method modifies the Occlusion method from
        captum~\url{https://captum.ai/api/occlusion.html} by only masking the last input data in time, preserving the
        previous inputs.
    \item \textbf{TemporalAugmentedOcclusion}.
        This method combines TemporalOcclusion and AugmentedOcclusion.
        As such, it only masks the last input in time, replacing it with samples from a bootstrapped distribution.
    \item \textbf{TemporalIntegratedGradients}.
        Novel method.
        Please see Section~\ref{sec:methods} for more details.
    \item \textbf{TimeForwardTunnel}.
        Novel method.
        Please see Section~\ref{sec:methods} for more details.

\end{itemize}


\paragraph{Datasets}

As part of \texttt{\detokenize{time_interpret}}, we include a set of datasets which can be readily used:

\begin{lstlisting}[language=Python, caption=Dataset loading example, label={lst:datasets}]
from tint.datasets import Arma

arma = Arma()
arma.download()  # This method generates the dataset

inputs = arma.preprocess()["x"]
true_saliency = arma.true_saliency(dim=1)
\end{lstlisting}

All these datasets are implemented using the DataModule from PyTorch-lightning.
We provide the following datasets:

\begin{itemize}
    \item \textbf{Arma}.
        This dataset was introduced by~\citep{crabbe2021explaining}.
        It relies on an ARMA process to generate time features $x_{ti}$, which are inputs combined by white-box
        regressor f.
        This regressor uses only part of the data, so we can know in advance which feature is salient, in order to
        evaluate a features attributions method.
        As a result, this dataset can be used to provide an initial comparison of various methods on time-series data.
    \item \textbf{BioBank}.
        BioBank~\citep{sudlow2015uk} is a large dataset based on the UK population.
        As such, it can be used to predict a specific condition based on a number of patients' data.
        In the default setting, this dataset can be used to train a model to predict the risk of developing type-II
        diabetes, but other conditions could be predicted.
        We also provide a cutoff before the onset of the condition, only using feature before that time, as well as
        a script to train FastText~\citep{bojanowski2017enriching} embeddings on the medical codes.
        The dataset can also be discretized, grouping medical codes into time intervals such as years.
        To access the data, a formal application must be made, please see
        https://www.ukbiobank.ac.uk/enable-your-research.
    \item \textbf{Hawkes}.
        Hawkes processes are a specific type of temporal point processes (TPP).
        The probability of an event of type k happening at time t is conditioned by a specific intensity function:
        \[ \lambda^*_k(t) = \mu_k + \sum_{n=1}^K \alpha_{kn} \sum_{t_i^n < t} \exp \left[ -\beta_{kn} (t - t_i^n) \right] \]
        With $\bm{\alpha}$, $\bm{\beta}$ and $\bm{\mu}$ being parameters of the process.
        As a result, an event has greater chance of happening if an event already happened previously, depending on the
        values of $\bm{\alpha}$ and $\bm{\beta}$.
        If no event previously happened, the base probability of an event happening is determined by $\bm{\mu}$.
        In this case, this intensity can therefore be used as the true saliency of the features.
        An important consequence for this dataset is that the true saliency is \textbf{dependent in time}.
        Indeed, an event can be very important for another one close in time, but also irrelevant for other events.
        As a result, the true saliency has two temporal dimensions: for each time, it gives the importance of each
        temporal event.
    \item \textbf{Hidden Markov model (HMM)}.
        This dataset was introduced by~\citep{crabbe2021explaining}.
        It consists in a 2-state hidden Markov model, with a features vector of size 3.
        For each time, only one of the three features conditions the label: either the second or the third, depending
        on the state.
        The first feature is always not salient.
        For this dataset, the true salient features are therefore also known.
    \item \textbf{Mimic-III}.
        Mimic-III~\citep{johnson2016mimic} consists of ICU data points.
        This dataset provides two tasks: predicting in-hospital mortality (static label) and predicting the
        average blood pressure (temporal label).
        The processing of this dataset was done following~\citep{tonekaboni2020went, crabbe2021explaining}.
\end{itemize}

\paragraph{Metrics}

\paragraph{Models}
